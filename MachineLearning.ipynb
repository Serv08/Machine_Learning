{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0a0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hello World\n",
    "'''\n",
    "print(\"Hello World\")\n",
    "'''\n",
    "ave = \"kris\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0209c3f3",
   "metadata": {},
   "source": [
    "<h1>Machine Learning</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cd17c2b",
   "metadata": {},
   "source": [
    "<h2>Linear Regression</h2>\n",
    "Slope formula is used in linear regression <br>\n",
    "m = (y_2 - y_1)/(x_1 - x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as pyplot\n",
    "import pickle       # pickle will be used to save the desired model (>=90% accuracy)\n",
    "from matplotlib import style\n",
    "\n",
    "data = pd.read_csv(\"student-mat.csv\", sep=';')\n",
    "# print(data.head())\n",
    "\n",
    "data = data[[\"G1\", 'G2', 'G3', 'studytime', 'failures', 'absences']]    # Data chosen should be integer\n",
    "# print(data.head())\n",
    "\n",
    "# LABELS\n",
    "predict = \"G3\"      # G3 (final grade) is wwhat we want to predict, thus labeling G3 as 'predict'\n",
    "\n",
    "x = np.array(data.drop(columns='G3'))       # data inputs // # x = np.array(data.drop([predict], 1))\n",
    "y = np.array(data[predict])                 # output\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.2)      # creates 4 outputs data set\n",
    "# x_train and y_train are the data that will be used to train the model while x_test and y_test will test the accuracy of that model\n",
    "# model_selection randomizes arrays and matrices into subsets for training and testing of data\n",
    "# train_test_split splits the data into train and test sets\n",
    "# test_size configures the size of the data that will be tested, in this case, 20% is used for test while 80% is used for training\n",
    "# print(f\"x_train:\\t{x_train}\\tx_test:\\t{x_test}\\ny_train:\\t{y_train}\\ty_test:\\t{y_test}\")\n",
    "\n",
    "## COMMENTING THIS OUT TO AFTER SAVING THE DESIRED MODEL\n",
    "# best = 0\n",
    "# while best < 0.95:    # while loop is used to achieve >= 95 % accuracy model\n",
    "#     x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.2)      \n",
    "    \n",
    "#     linear = LinearRegression()\n",
    "#     # linear = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "#     linear.fit(x_train, y_train)        # .fit for best fit line\n",
    "#     acc = linear.score(x_test, y_test)  # .score test the accuracy\n",
    "#     print('Accurancy: \\t', round(acc*100,2),'%')    # this is a modified line of code that prints acc\n",
    "\n",
    "#     # Creates a pickle file\n",
    "#     # if acc > best:\n",
    "#     best = acc\n",
    "#     with open(\"studentmodel.pickle\", \"wb\") as f:        # \"wb\" is binary format for WRITING within the file\n",
    "#         pickle.dump(linear, f)\n",
    "##\n",
    "\n",
    "# print('Accurancy: \\t', round(acc*100,2),'%')    # this is a modified line of code that prints acc\n",
    "\n",
    "# Opens the pickle file created\n",
    "pickle_in = open(\"studentmodel.pickle\", \"rb\" )      # \"rb\" is binary format for READING the file\n",
    "linear = pickle.load(pickle_in)     # loads the pickle_in file within the variable linear\n",
    "\n",
    "print('Coefficients: \\t', linear.coef_)     # B (coefficient)\n",
    "print('Intercept: \\t', linear.intercept_)   # Y-intercept\n",
    "\n",
    "predictions = linear.predict(x_test)\n",
    "print('\\nPredictions\\tData\\t\\t\\tActual Grade')\n",
    "for x in range(len(predictions)):\n",
    "    print(round(predictions[x],7), x_test[x], y_test[x], sep='\\t')\n",
    "\n",
    "p = 'absences'\n",
    "style.use(\"ggplot\")\n",
    "pyplot.scatter(data[p], data[predict])\n",
    "pyplot.xlabel(p)\n",
    "pyplot.ylabel(\"Final Grade\")\n",
    "pyplot.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91499c23",
   "metadata": {},
   "source": [
    "<h2>KNN</h2>\n",
    "K-nearest neighbors can be used to predict the classification of a specified unknown point in a grapah given the data available.<br>\n",
    "K is known as the <i>hyper parameter</i> that stands for amount of neighbors.<br>\n",
    "Those neighbors will then be used to predict what is the classification of that specified unknown point.<br>\n",
    "Distance formula is used in KNN to determine the closest neighbors<br>\n",
    "d = sqrt((x_2-x_1)^2 + (y_2-y_1)^2)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model, preprocessing\n",
    "\n",
    "data = pd.read_csv(\"car.data\")\n",
    "# print(data.head())\n",
    "\n",
    "le = preprocessing.LabelEncoder()       # turns non-numerical data into numerical ones\n",
    "\n",
    "# Turns the specified column into a list with the help of .fit_transform\n",
    "buying = le.fit_transform(list(data[\"buying\"]))\n",
    "maint = le.fit_transform(list(data[\"maint\"]))\n",
    "door = le.fit_transform(list(data[\"door\"]))\n",
    "persons = le.fit_transform(list(data[\"persons\"]))\n",
    "lug_boot = le.fit_transform(list(data[\"lug_boot\"]))\n",
    "safety = le.fit_transform(list(data[\"safety\"]))\n",
    "cls = le.fit_transform(list(data[\"class\"]))\n",
    "# print(buying) \n",
    "\n",
    "predict = \"class\"\n",
    "\n",
    "X = list(zip(buying, maint, door, persons, lug_boot, safety))       # converts into a whole list\n",
    "Y = list(cls)       # Y value is the class\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size = 0.1)      # test_size is 0.1 where 90% of the data is for training and 10% of the data is for testing\n",
    "# print(x_train, y_test)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=10)      # takes the amount of neighbors as parameters\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "acc = model.score(x_test, y_test)\n",
    "print('Accurancy: \\t', round(acc*100,2),'%')    # this is a modified line of code that prints acc\n",
    "\n",
    "predicted = model.predict(x_test)       # creates the model for KNN\n",
    "names = [\"unacc\", \"acc\", \"good\", \"vgood\"]   # takes the acutal value of the names that corresponds to the specific index, \"unacc\" for 0, \"acc\" for 1, etc...\n",
    "\n",
    "print(\"Predicted:\\tData:\\t\\t\\tActual:\")\n",
    "for x in range(len(predicted)):\n",
    "    print(f\"{predicted[x]}\\t\\t{x_test[x]}\\t{y_test[x]}\\t{names[y_test[x]]}\")\n",
    "    n = model.kneighbors([x_test[x]],9,True)    # returns the distance to those neighbors \n",
    "    # print(\"N: \", n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8b39ca9",
   "metadata": {},
   "source": [
    "<h2>Sklearn Datasets</h2>\n",
    "<h3>Support Vector Machines (SVM) Algorithm</h3>\n",
    "<b>Hyperplane</b> is where two points from different sets have the same distance relative to a line.<br>\n",
    "<b>Margin</b> is the distance between the hyperplane and the observations closest to the hyperplane. There is soft margin and hard margin.<br>\n",
    "The difference between a <i>hard margin</i> and a <i>soft margin</i> in SVMs lies in the separability of the data. If our data is linearly separable, we go for a hard margin. If the want to allow some outliers inside the margin, we can go for soft margin. </br>\n",
    "<b>Kernel</b> is a function where two inputs, i.e. x_1 and x_2, are used to generate an output, x_3, that is a higher dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0a5bcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurancy: \t 93.01 %\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "# print(\"Features:\\n\",cancer.feature_names)\n",
    "# print(\"Result:\\n\", cancer.target_names)\n",
    "\n",
    "x = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.25)      # test_size is 0.25 where 75% of the data is for training and 25% of the data is for testing\n",
    "# print(x_train, y_train)\n",
    "\n",
    "result = ['malignant', 'benign']    # the index value will be used, i.e. 0 and 1\n",
    "\n",
    "# clf = svm.SVC()                   # implementing classifier using Support Vector Classificatio\n",
    "# clf = svm.SVC(kernel=\"linear\", C=2)      # linear; C is the soft margin that determines the number of outliers allowed in the margin\n",
    "# clf = svm.SVC(kernel=\"poly\", degree = 2)  # polynomial; works on exponents\n",
    "clf = KNeighborsClassifier(n_neighbors = 13)    # using KNN for best fit\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Accurancy: \\t', round(acc*100,2),'%')    # this is a modified line of code that prints acc\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "525dc924",
   "metadata": {},
   "source": [
    "<h1>K Means Clustering</h1>     \n",
    "K means clustering is an unsupervised learning algorithm<br>\n",
    "<i>Centroids</i> determine cluster points<br>\n",
    "<i>Setting Centroids:</i><br>\n",
    "\n",
    "1. Place random centroids<br>\n",
    "2. Draw line perpendicular to the  midpoint of the two centroids<br>\n",
    "3. Classify the plots that are covered by the line drawn<br>\n",
    "4. Place the Centroids to the middle of the respected classified plots<br>\n",
    "5. Repeat step 2-4 until there no more change in the data<br><br>\n",
    "Cons:<br>\n",
    "\n",
    "* Speed - lot of calculation for each of the single data point since distance is solved for the specified number of neighbors.<br>\n",
    "    * P x C x F where P is the data point, C is the number of centroids, and F is the features or the number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "digits = load_digits()\n",
    "data = scale(digits.data)\n",
    "y = data.targets\n",
    "\n",
    "k = 10      # len(np.unique(y)) -- to get the amound of different classes/classifications for data sets dynamically \n",
    "samples, features = data.shape\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    estimator.fit(data)\n",
    "    print('%-9s\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "          % (name, estimator.inertia_,\n",
    "             metrics.homogeneity_score(y, estimator.labels_),\n",
    "             metrics.completeness_score(y, estimator.labels_),\n",
    "             metrics.v_measure_score(y, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(y, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(y,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean')))\n",
    "\n",
    "clf = KMeans()      # classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ef995eb2be3b9917ba3cf75e584d90927da4ba2ec042ac666d9afc6a2c9cde7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
